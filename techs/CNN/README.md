# 1. Understanding Convolutions

* CNN 原理解释





# 2. CNN 结构


## 1. CNN结构概述
### 1. 常规的神经网络--- 全连接：
  * 每个神经元与前一层的所有神经元连接，同一个隐层的神经元相互独立，---全连接层
  * 常规神经网络在大尺寸图像上 参数过多，效率低下，可能导致过拟合
  * 不能描述 局部不变性特征 -- 平移 缩放

### 2.卷积神经网络：--- 卷积层
  * 三维排列
  * 神经元只对应前一层的一小块区域连接----不是全连接

<div align="center">  <img src="https://github.com/LiuChuang0059/ComplexNetwork-DataMining/blob/master/techs/Image/%E5%8D%B7%E7%A7%AF%E5%B1%82-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82.png" width="800"/> </div><br>

* 连接数 从原来的 n_l * n_(l-1) 变为 n_l* m （m 为滤波器的大小）

-----
-----


## 2.  构建CNN的各种层

### 1. 卷积层
> 图像本身具有“二维空间特征”，通俗点说就是局部特性。譬如我们看一张猫的图片，可能看到猫的眼镜或者嘴巴就知道这是张猫片，而不需要说每个部分都看完了才知道，啊，原来这个是猫啊。所以如果我们可以用某种方式对一张图片的某个典型特征识别，那么这张图片的类别也就知道了

##### 1.  卷积层由一些可以学习的滤波器集合构成
  * 滤波器的宽度和高度都比较小，深度和输入数据一致。----滤波器的尺寸选取为什么是奇数
  * 滤波器在输入数据的宽度和高度上滑动---产生一个二维的激活图
  * 将所有的激活映射在深度方向上 层叠 起来生成了输出数据

##### 2.  局部连接
  * 神经元只和输入数据的一个局部区域连接---receptive field
  * 参数数量： [5x5x3]---5x5x3+1(偏置参数)=76

  <div align="center">  <img src="https://github.com/LiuChuang0059/ML_Project/blob/master/Picture/%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%B1%80%E9%83%A8%E8%BF%9E%E6%8E%A5.jpg" width="800"/> </div><br>


##### 3. 输出空间排列---输出数据体的神经元数量

  * 输出的深度： 深度和使用的滤波器的数量(Filter0,Filter1...)一致；沿着深度方向排列、感受野相同的神经元集合称为深度列

  * 步长（stride）： 当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素

    > （N-F+2P）/s +1  一定要是整数

  * 零填充（zero-padding）：控制输出的数据体的空间尺寸： 补充（F-1）/2 --F：尺寸 可以保持原图的大小

    > 为什么使用0进行补充，因为0 对卷积输出没贡献,不填充，边缘信息损失

  * 输出解析

<div align="center">  <img src="https://github.com/LiuChuang0059/ML_Project/blob/master/Picture/%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E5%9B%BE%E8%A7%A3.jpg" width="800"/> </div><br>

> 通过第一个卷积核计算后的feature_map是一个三维数据，在第三列的绝对值最大，说明原始图片上对应的地方有一条垂直方向的特征，即像素数值变化较大；而通过第二个卷积核计算后，第三列的数值为0，第二行的数值绝对值最大，说明原始图片上对应的地方有一条水平方向的特征。

-----------

### 2. Pooling 层
* 最大 pooling ---较好
* 平均汇聚
* 范式汇聚

* 进行Max Pooling操作后，提取出的是真正能够识别特征的数值，其余被舍弃的数值，对于我提取特定的特征并没有特别大的帮助

* 减小了feature map的尺寸，从而减少参数，达到减小计算量，缺不损失效果的情况。

* 直观上，一旦一个特征被发现，它的确切位置并不如它相对于其它特征的大概位置重要。一个很大的好处是，这样可以有很多被更少地混合的特征，所以这有助于减少在以后的层所需的参数的数目

<div align="center">  <img src="https://github.com/LiuChuang0059/ML_Project/blob/master/Picture/%E6%B1%87%E8%81%9A%E5%B1%82.jpg" width="800"/> </div><br>


> 不使用汇聚层：很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，可能会很少使用甚至不使用汇聚层

---------

### 3.Flatten 层


<div align="center">  <img src="https://github.com/LiuChuang0059/ML_Project/blob/master/Picture/flatten%E5%B1%82%E8%BF%87%E7%A8%8B.jpg" width="800"/> </div><br>

------


### 4. 全连接层



##### 全连接层转化为卷积层

> 全连接层转化为卷积层：在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是224x224x3的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：

> 针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7，这样输出数据体就为[1x1x4096]了。
针对第二个全连接层，令其滤波器尺寸为F=1，这样输出数据体为[1x1x4096]。
对最后一个全连接层也做类似的，令其F=1，最终输出为[1x1x1000]
实际操作中，每次这样的变换都需要把全连接层的权重W重塑成卷积层的滤波器。那么这样的转化有什么作用呢？它在下面的情况下可以更高效：让卷积网络在一张更大的输入图片上滑动（即把一张更大的图片的不同区域都分别带入到卷积网络，得到每个区域的得分），得到多个输出，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。

> 举个例子，如果我们想让224x224尺寸的浮窗，以步长为32在384x384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6x6个位置的类别得分。上述的把全连接层转换成卷积层的做法会更简便。如果224x224的输入图片经过卷积层和汇聚层之后得到了[7x7x512]的数组，那么，384x384的大图片直接经过同样的卷积层和汇聚层之后会得到[12x12x512]的数组（因为途径5个汇聚层，尺寸变为384/2/2/2/2/2 = 12）。然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出（因为(12 - 7)/1 + 1 = 6）。这个结果正是浮窗在原图经停的6x6个位置的得分！

> 面对384x384的图像，让（含全连接层）的初始卷积神经网络以32像素的步长独立对图像中的224x224块进行多次评价，其效果和使用把全连接层变换为卷积层后的卷积神经网络进行一次前向传播是一样的。
自然，相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。这一技巧在实践中经常使用，一次来获得更好的结果。比如，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。

> 最后，如果我们想用步长小于32的浮窗怎么办？用多次的向前传播就可以解决。比如我们想用步长为16的浮窗。那么先使用原图在转化后的卷积网络执行向前传播，然后分别沿宽度，沿高度，最后同时沿宽度和高度，把原始图片分别平移16个像素，然后把这些平移之后的图分别带入卷积网络。




-------


### 6. 层卷积运算演示


**卷积层运算演示**
[动画演示link](http://cs231n.github.io/convolutional-networks/)

----------
-------


## 3. 结构特性

### 1. 局域感受野
 * 隐藏神经元对应的区域 称为 局域感受野



-----

### 2. 参数共享-- 同一个卷积核(滤波器)

**每个切片只有一个权重集---每个深度切片中的神经元使用同样的权重和参数**

  * 如果在一个深度切片中的所有权重都使用同一个权重向量，那么卷积层的前向传播在每个深度切片中可以看做是在计算神经元权重和输入数据体的卷积

> 为什么取同样的参数：
>>> 作一个合理的假设：如果一个特征在计算某个空间位置(x,y)的时候有用，那么它在计算另一个不同位置(x2,y2)的时候也有用。基于这个假设，可以显著地减少参数数量

>>> 把权重和偏置设想成隐藏神经元可以挑选的东西，例如，在一个特定的局部感受野的垂直边缘。这种能力在图像的其它位置也很可能是有用的。因此，在图像中应用相同的特征检测器是非常有用的。卷积网络能很好地适应图像的平移不变性


**有时候参数共享假设可能没有意义，特别是当卷积神经网络的输入图像是一些明确的中心结构时候。这时候我们就应该期望在图片的不同位置学习到完全不同的特征。一个具体的例子就是输入图像是人脸，人脸一般都处于图片中心。你可能期望不同的特征，比如眼睛特征或者头发特征可能（也应该）会在图片的不同位置被学习。在这个例子中，通常就放松参数共享的限制，将层称为局部连接层**

<div align="center">  <img src="https://github.com/LiuChuang0059/ML_Project/blob/master/Picture/%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E4%BE%8B%E5%AD%90.png" width="400"/> </div><br>


* 假如 有3 个filter ，或者说有三组不同 的共享权重和偏置， 则可以提取图片的 三组不同的特征，每个特征都可以在整个图像中检测。

----


### 3.子采样-- pooling

* 卷积层虽然可以显著减少网络中连接的数量，但特征映射组中的神经元个 数并没有显著减少

* 在卷积层之后加上一个汇聚层，从 而降低特征维数，避免过拟合。



-------
-------

## 4. 典型的卷积网络结构


<div align="center">  <img src="https://github.com/LiuChuang0059/ComplexNetwork-DataMining/blob/master/techs/Image/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png" width="800"/> </div><br>



* 一个卷积块为连续 M 个卷积层和 b 个汇聚层(M 通常设置为2 ∼ 5，b为0或1)。一个卷积网络中可以堆叠N个连续的卷积块， 然后在接着K个全连接层(N的取值区间比较大，比如1 ∼ 100或者更大;K 一般为0 ∼ 2)。


* 整个网络结构趋向于使用更小的卷积核(比如1 × 1和3 × 3)以及 更深的结构(比如层数大于 50)。此外，由于卷积的操作性越来越灵活(比如不 同的步长)，汇聚层的作用变得也越来越小，因此目前比较流行的卷积网络中， 汇聚层的比例也逐渐降低，趋向于全卷积网络。








------
------

## 5. 参数选取

### 1. 层的尺寸设计

* 输入层（包含图像的）应该能被2整除很多次

* 卷积层应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长S=1

> 因为内存限制所做的妥协：在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。


### 2.卷积核的个数如何确定

> 由经验确定。通常情况下，靠近输入的卷积层，譬如第一层卷积层，会找出一些共性的特征，如手写数字识别中第一层我们设定卷积核个数为5个，一般是找出诸如"横线"、“竖线”、“斜线”等共性特征，我们称之为basic feature，经过max pooling后，在第二层卷积层，设定卷积核个数为20个，可以找出一些相对复杂的特征，如“横折”、“左半圆”、“右半圆”等特征，越往后，卷积核设定的数目越多，越能体现label的特征就越细致，就越容易分类出来


-----
------



# 3. 典型的 CNN 网络

## AlexNet[Krizhevsky et al., 2012]是第一个现代深度卷积网络模型

## Inception 网络



## 残差网络(Residual Network，ResNet)

------
-----









# 参考

* Understanding Convolutions ----- http://colah.github.io/posts/2014-07-Understanding-Convolutions/

* [cs231 课堂笔记--官网英文原版](http://cs231n.github.io/convolutional-networks/)
> 动图展示很直观

* [Neural Networks and Deep Learning---书籍网页](http://neuralnetworksanddeeplearning.com/index.html)

* [邱锡鹏:《神经网络与深度学习](https://nndl.github.io/)

* [《神经网络和深度学习》系列文章四十四：介绍卷积网络](https://mp.weixin.qq.com/s?__biz=MzIxMjAzNDY5Mg==&mid=2650790918&idx=1&sn=4fad7df685991d3dbbd2469fe8c5aedf&scene=21#wechat_redirect)

* [《神经网络和深度学习》系列文章四十五：卷积神经网络在实际中的应用](https://mp.weixin.qq.com/s?__biz=MzIxMjAzNDY5Mg==&mid=2650790932&idx=1&sn=10c8c5ee729d1fa200d2dd189f5edc38&chksm=8f4749ffb830c0e9a0efb8a6f944ba49c3419c995d8a07885f30afd3a0743040b266b5751255&scene=21#wechat_redirect)


* [keras 手把手入门#1-MNIST手写数字识别](http://nooverfit.com/wp/keras-%E6%89%8B%E6%8A%8A%E6%89%8B%E5%85%A5%E9%97%A81-%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/)



* [keras学习之五：Keras中的神经网络层组件简介](https://blog.csdn.net/zzulp/article/details/76590712)


* [cnn实现手写识别code+网络层组件学习](https://github.com/LiuChuang0059/ML_Project/blob/master/code/mnist_cnn_keras.ipynb)

* dropout避免过拟合---http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf

* [手写数字识别--卷积神经网络CNN原理详解](https://zhuanlan.zhihu.com/p/30665319)---by Charlotte

* [如何用卷积神经网络CNN识别手写数字集？](https://www.cnblogs.com/charlotte77/p/5671136.html)----by Charlotte



